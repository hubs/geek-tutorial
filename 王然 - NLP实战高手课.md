NLP实战高手课 / 全方位提升你的NLP实战技能
=========================

王然 **众微科技AI Lab技术负责人、阿姆斯特丹大学数学和计量经济学双硕士**
-----------------------------------------

![NLP实战高手课](https://www.geekgay.com/storage/geek/geek_e9ce5f84cd8bcf96fb341690cb56562e.jpg)  
  
👉👉[**订阅返现**](https://time.geekbang.org/course/intro/100046401?code=%2FsTJkebkHO9cjtkh4AfKueWaqf6yzhYqekO2jeUNiVk%3D "NLP实战高手课")  
  
课程目录
----

  
  
- 06 | NLP应用：智能问答系统
- 07 | NLP应用：文本校对系统
- 08 | NLP的学习方法：如何在AI爆炸时代快速上手学习？
- 09 | 深度学习框架简介：如何选择合适的深度学习框架？
- 10 | 深度学习与硬件：CPU
- 11 | 深度学习与硬件：GPU
- 12 | 深度学习与硬件：TPU
- 13 | AI项目部署：基本原则
- 14 | AI项目部署：框架选择
- 15 | AI项目部署：微服务简介
- 16 | 统计学基础：随机性是如何改变数据拟合的本质的？
- 17 | 神经网络基础：神经网络还是复合函数
- 18 | 神经网络基础：训练神经网络
- 19 | 神经网络基础：神经网络的基础构成
- 20 | Embedding简介：为什么Embedding更适合编码文本特征？
- 21 | RNN简介：马尔可夫过程和隐马尔可夫过程
- 22 | RNN简介：RNN和LSTM
- 23 | CNN：卷积神经网络是什么？
- 24 | 环境部署：如何构建简单的深度学习环境？
- 25 | PyTorch简介：Tensor和相关运算
- 26 | PyTorch简介：如何构造Dataset和DataLoader？
- 27 | PyTorch简介：如何构造神经网络？
- 28 | 文本分类实践：如何进行简单的文本分类？
- 29 | 文本分类实践的评价：如何提升进一步的分类效果？
- 30 | 经典的数据挖掘方法：数据驱动型开发早期的努力
- 31 | 表格化数据挖掘基本流程：看看现在的数据挖掘都是怎么做的？
- 32 | Pandas简介：如何使用Pandas对数据进行处理？
- 33 | Matplotlib简介：如何进行简单的可视化分析？
- 34 | 半自动特征构建方法：Target Mean Encoding
- 35 | 半自动特征构建方法：Categorical Encoder
- 36 | 半自动特征构建方法：连续变量的离散化
- 37 | 半自动特征构建方法：Entity Embedding
- 38 | 半自动构建方法：Entity Embedding的实现
- 39 | 半自动特征构建方法：连续变量的转换
- 40 | 半自动特征构建方法：缺失变量和异常值的处理
- 41 | 自动特征构建方法：Symbolic learning和AutoCross简介
- 42 | 降维方法：PCA、NMF 和 tSNE
- 43 | 降维方法：Denoising Auto Encoders
- 44 | 降维方法：Variational Auto Encoder
- 45 | 变量选择方法
- 46 | 集成树模型：如何提升决策树的效果
- 47 | 集成树模型：GBDT和XgBoost的数学表达
- 48 | 集成树模型：LightGBM简介
- 49 | 集成树模型：CatBoost和NGBoost简介
- 50 | 神经网络建模：如何让神经网络实现你的数据挖掘需求
- 51 | 神经网络的构建：Residual Connection和Dense Connection
- 52 | 神经网络的构建：Network in Network
- 53 | 神经网络的构建：Gating Mechanism和Attention
- 54 | 神经网络的构建：Memory
- 55 | 神经网络的构建：Activation Function
- 56 | 神经网络的构建：Normalization
- 57 | 神经网络的训练：初始化
- 58 | 神经网络的训练：学习率和Warm-up
- 59 | 神经网络的训练：新的PyTorch训练框架
- 60 | Transformer：如何通过Transformer榨取重要变量？
- 61 | Transformer代码实现剖析
- 62 | xDeepFM：如何用神经网络处理高维的特征？
- 63 | xDeepFM的代码解析
- 64 | 时序建模：如何用神经网络解决时间序列的预测问题？
- 65 | 图嵌入：如何将图关系纳入模型？
- 66 | 图网络简介：如何在图结构的基础上建立神经网络？
- 67 | 模型融合基础：如何让你所学到的模型方法一起发挥作用？
- 68 | 高级模型融合技巧：Metades是什么？
- 69 | 挖掘自然语言中的人工特征：如何用传统的特征解决问题？
- 70 | 重新审视Word Embedding：Negative Sampling和Contextual Embedding
- 71 | 深度迁移学习模型：从ELMo到BERT
- 72 | 深度迁移学习模型：RoBERTa、XLNet、ERNIE和T5
- 73 | 深度迁移学习模型：ALBERT和ELECTRA
- 74 | 深度迁移学习模型的微调：如何使用TensorFlow在TPU对模型进行微调
- 75 | 深度迁移学习模型的微调：TensorFlow BERT代码简析
- 76 | 深度迁移学习的微调：如何利用PyTorch实现深度迁移学习模型的微调及代码简析
- 77 | 优化器：Adam和AdamW
- 78 | 优化器：Lookahead，Radam和Lamb
- 79 | 多重loss的方式：如何使用多重loss来提高模型准确率？
- 80 | 数据扩充的基本方法：如何从少部分数据中扩充更多的数据并避免过拟合？
- 81 | UDA：一种系统的数据扩充框架
- 82 | Label Smoothing和Logit Squeezing
- 83 | 底层模型拼接：如何让不同的语言模型融合在一起从而达到更好的效果？
- 84 | 上层模型拼接：如何在语言模型基础上拼接更多的模型？
- 85 | 长文本分类：截取、关键词拼接和预测平均
- 86 | Virtual Adverserial Training：如何减少一般对抗训练难收敛的问题并提高结果的鲁棒性？
- 87 | 其他Embedding的训练：还有哪些Embedding方法？
- 88 | 训练预语言模型
- 89 | 多任务训练：如何利用多任务训练来提升效果？
- 90 | Domain Adaptation：如何利用其它有标注语料来提升效果？
- 91 | Few-shot Learning：是否有更好的利用不同任务的方法？
- 92 | 半监督学习：如何让没有标注的数据也派上用场？
- 93 | 依存分析和Semantic Parsing概述
- 94 | 依存分析和Universal Depdency Relattions
- 95 | 如何在Stanza中实现Dependency Parsing
- 96 | Shift Reduce算法
- 97 | 基于神经网络的依存分析算法
- 98 | 树神经网络：如何采用Tree LSTM和其它拓展方法？
- 99 | Semantic Parsing基础：Semantic Parsing的任务是什么？
- 100 | WikiSQL任务简介
- 101 | ASDL和AST
- 102 | Tranx简介
- 103 | Lambda Caculus概述
- 104 | Lambda-DCS概述
- 105 | Inductive Logic Programming：基本设定
- 106 | Inductive Logic Programming：一个可微的实现
- 107 | 增强学习的基本设定：增强学习与传统的预测性建模有什么区别？
- 108 | 最短路问题和Dijkstra Algorithm
- 109 | Q-learning：如何进行Q-learning算法的推导？
- 110 | Rainbow：如何改进Q-learning算法？
- 111 | Policy Gradient：如何进行Policy Gradient的基本推导？
- 112 | A2C和A3C：如何提升基本的Policy Gradient算法
- 113 | Gumbel-trick：如何将离散的优化改变为连续的优化问题？
- 114 | MCTS简介：如何将“推理”引入到强化学习框架中
- 115 | Direct Policty Gradient：基本设定及Gumbel-trick的使用
- 116 | Direct Policty Gradient：轨迹生成方法
- 117 | AutoML及Neural Architecture Search简介
- 118 | AutoML网络架构举例
- 119 | RENAS：如何使用遗传算法和增强学习探索网络架构
- 120 | Differentiable Search：如何将NAS变为可微的问题
- 121 | 层次搜索法：如何在模块之间进行搜索？
- 122 | LeNAS：如何搜索搜索space
- 123 | 超参数搜索：如何寻找算法的超参数
- 124 | Learning to optimize：是否可以让机器学到一个新的优化器
- 125 | 遗传算法和增强学习的结合
- 126 | 使用增强学习改进组合优化的算法
- 127 | 多代理增强学习概述：什么是多代理增强学习？
- 128 | AlphaStar介绍：AlphaStar中采取了哪些技术？
- 129 | IMPALA：多Agent的Actor-Critic算法
- 130 | COMA:Agent之间的交流
- 131 | 多模态表示学习简介
- 132 | 知识蒸馏：如何加速神经网络推理
- 133 | DeepGBM：如何用神经网络捕捉集成树模型的知识
- 134 | 文本推荐系统和增强学习
- 135 | RL训练方法集锦：简介
- 136 | RL训练方法:RL实验的注意事项
- 137 | PPO算法
- 138 | Reward设计的一般原则
- 139 | 解决Sparse Reward的一些方法
- 140 | Imitation Learning和Self-imitation Learning
- 141 | 增强学习中的探索问题
- 142 | Model-based Reinforcement Learning
- 143 | Transfer Reinforcement Learning和Few-shot Reinforcement Learning
- 144 | Quora问题等价性案例学习：预处理和人工特征
- 145 | Quora问题等价性案例学习：深度学习模型
- 146 | 文本校对案例学习
- 147 | 微服务和Kubernetes简介
- 148 | Docker简介
- 149 | Docker部署实践
- 150 | Kubernetes基本概念
- 151 | Kubernetes部署实践
- 152 | Kubernetes自动扩容
- 153 | Kubernetes服务发现
- 154 | Kubernetes Ingress
- 155 | Kubernetes健康检查
- 156 | Kubernetes灰度上线
- 157 | Kubernetes Stateful Sets
- 158 | Istio简介：Istio包含哪些功能？
- 159 | Istio实例和Circuit Breaker
- 160 | 结束语